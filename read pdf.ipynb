{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMsJEAInnUErVg7brn933y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marshall1632/LLM/blob/main/read%20pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3gex_PQezDm8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in2WLikSJEm6",
        "outputId": "f82b4108-f653-4307-c390-1a694a7c0ed1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfx\n",
            "  Downloading pdfx-1.4.1-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting pdfminer.six==20201018 (from pdfx)\n",
            "  Downloading pdfminer.six-20201018-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting chardet==4.0.0 (from pdfx)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20201018->pdfx) (43.0.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20201018->pdfx) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->pdfminer.six==20201018->pdfx) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20201018->pdfx) (2.22)\n",
            "Downloading pdfx-1.4.1-py2.py3-none-any.whl (21 kB)\n",
            "Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20201018-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chardet, pdfminer.six, pdfx\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "Successfully installed chardet-4.0.0 pdfminer.six-20201018 pdfx-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfx"
      ],
      "metadata": {
        "id": "Ak5_M_QTJOt4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = pdfx.PDFx(\"https://arxiv.org/pdf/2308.03171\")\n",
        "pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3ZP40DANGku",
        "outputId": "284fd611-36d5-4672-821d-a14b8249e22a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pdfx.PDFx at 0x7f26a30a1750>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= pdf.get_text()\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ezm9klbjNXnD",
        "outputId": "fd4018fb-9047-4f2d-f97a-d374d7006b2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3\\n2\\n0\\n2\\n \\ng\\nu\\nA\\n \\n6\\n \\n \\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n1\\n7\\n1\\n3\\n0\\n.\\n8\\n0\\n3\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nDetection of Anomalies in Multivariate Time Series\\nUsing Ensemble Techniques\\n\\nAnastasios Iliopoulos\\nDept. Informatics & Telematics\\nHarokopio University of Athens\\nAthens, Greece\\nitp20152@hua.gr\\n\\nChristos Diou\\nDept. Informatics & Telematics\\nHarokopio University of Athens\\nAthens, Greece\\ncdiou@hua.gr\\n\\nJohn Violos\\nDept. Informatics & Telematics\\nHarokopio University of Athens\\nAthens, Greece\\nviolos@hua.gr\\n\\nIraklis Varlamis\\nDept. Informatics & Telematics\\nHarokopio University of Athens\\nAthens, Greece\\nvarlamis@hua.gr\\n\\nAbstract—Anomaly Detection in multivariate time series is a\\nmajor problem in many fields. Due to their nature, anomalies\\nsparsely occur in real data, thus making the task of anomaly\\ndetection a challenging problem for classification algorithms to\\nsolve. Methods that are based on Deep Neural Networks such\\nas LSTM, Autoencoders, Convolutional-Autoencoders, etc., have\\nshown positive results in such imbalanced data. However, the ma-\\njor challenge that algorithms face when applied to multivariate\\ntime series is that the anomaly can arise from a small subset of\\nthe feature set. To boost the performance of these base models, we\\npropose a feature-bagging technique that considers only a subset\\nof features at a time, and we further apply a transformation that\\nis based on nested rotation computed from Principal Component\\nAnalysis (PCA) to improve the effectiveness and generalization\\nof the approach. To further enhance the prediction performance,\\nwe propose an ensemble technique that combines multiple base\\nmodels toward the final decision. In addition, a semi-supervised\\napproach using a Logistic Regressor to combine the base models’\\noutputs is proposed. The proposed methodology is applied to the\\nSkoltech Anomaly Benchmark (SKAB) dataset, which contains\\ntime series data related to the flow of water in a closed circuit,\\nand the experimental results show that the proposed ensemble\\ntechnique outperforms the basic algorithms. More specifically,\\nthe performance improvement in terms of anomaly detection\\naccuracy reaches 2% for the unsupervised and at least 10% for\\nthe semi-supervised models.\\n\\nIndex Terms—Anomaly Detection, Ensemble Methods, Deep\\n\\nLearning, Time Series, Multivariate\\n\\nI. INTRODUCTION\\n\\nA significant portion of actual data from systems, phenom-\\nena, or measurements is represented by time series. The ever-\\nincreasing volume of data makes it challenging for humans to\\nmonitor and analyze data, and for this, they rely on algorithms\\nand machine learning models that automate such tasks and\\nimprove the monitoring process. In the case of time series,\\nmachine learning models try to learn the patterns behind the\\nevolution of a series and either predict future values or detect\\nabnormal situations when they occur. An anomaly is defined\\nas an observation or sequence of observations that deviate\\n\\nsignificantly from the distribution of the data, and they usually\\nconstitute a very small portion of the total data [1]. The anoma-\\nlies are encountered whenever something goes wrong during\\nthe evolution of an operation, a phenomenon, or a process\\nover time. It is crucial to identify these anomalous points\\nand automatically determine if the corresponding sequence\\nof values is an anomaly or not. Anomaly detection is linked\\nto several real-life applications, such as the identification of\\nfraudulent bank transactions, the early detection of machine\\nmalfunctioning, the detection of symptoms that indicate the\\nexistence of a disease or virus [2], and the detection of system\\nintrusions [3], just to mention a few.\\n\\nAnomaly detection can be approached mostly from two\\nperspectives. It can be approached as a binary classification\\nproblem [4] that classifies an observation as an anomaly\\nor not. It can also be approached as an outlier detection\\nproblem where we seek unusual patterns or values that are\\nfar from the majority of observations [5]. This study follows\\nthe first approach and examines anomaly detection as a binary\\nclassification problem. However, apart from the fact that by\\ndefinition anomalies are very sparse in a dataset, they can\\nalso vary significantly in type, which makes it very difficult\\nfor a standard classification algorithm to deal with. The\\nthree main anomaly types as described in [6] are: i) point\\nanomalies, which refer to anomalies that have extreme values,\\nii) collective anomalies, which correspond to individual points\\nthat have common values with many other points, but which\\nact very strangely as a group, and iii) context anomalies,\\nwhich refer to points that in some environments or context\\nare considered normal but in some other context are counted\\nas anomalies.\\n\\nWhile most anomaly detection models focus on a specific\\ntype of anomaly and neglect all other types, our proposed\\nensemble approach combines multiple models (weak models)\\nthat examine different types of anomaly and different features\\nof the data. The ensemble methods we used combines the\\n\\n\\x0coutputs of the weak models leveraging the diversity of their\\nabilities to capture anomalies in multiple features concurrently.\\nMore specifically, in the context of this work, a multi-step\\napproach has been designed, implemented, and experimentally\\nevaluated both in usupervised and semi-supervised setup. At\\nfirst, multiple models, also named learners, are trained, each\\none using a different, randomly chosen, subset of features.\\nThen a transformation is applied on each subset computed\\nfrom principal component analysis (PCA) to capture the\\nvariance of data and inject diversity.\\n\\nIn the unsupervised setup the basic models are combined us-\\ning the majority voting technique while in the semi-supervised\\nalternative a Logistic Regressor is used to combine the pre-\\ndictions and provide a final answer.\\n\\nThe multi-step approach allows the detection of anomalies\\neven when they are hidden in lower dimensions while at\\nthe same time, preserving the diversity of each learner. As\\na result, the ensembles can perform well in higher dimensions\\nand demonstrates an increased performance compared to the\\noriginal methods.\\n\\nThe major contributions of this paper are listed as follows:\\n• A method that applies Feature Bagging to the full set\\nof features is combined with multiple basic anomaly\\ndetectors that specialize in different anomaly types and\\nmanages to uncover anomalies hidden in subsets of\\nfeatures.\\n\\n• A combination with a transformation based on Principal\\nComponent Analysis to the resulting feature sets further\\nincreases the diversity of detectors and better captures the\\ndifferent types of anomaly.\\n\\n• The two feature selection and transformation techniques,\\nare combined with multiple detectors in an ensemble\\nmodel, which outperforms the baseline methods in terms\\nof prediction performance.\\n\\n• the methodological approach proposed in this work is\\nexperimentally validated on a popular anomaly detection\\ndataset.\\n\\nThe rest of this document is structured as follows: Section 2\\nbriefly surveys the literature on anomaly detection techniques\\nwith an emphasis on ensemble methods and their application\\non time series. Section 3 briefly formulates the challenge\\nthat we address in this study. Section 4 formally defines the\\nproposed approach and provides the details of the implemented\\nmodel. Section 5 describes the experimental evaluation and\\nprovides an interpretation of the results. Finally, Section 6\\nconcludes the paper and discusses the next steps of this work.\\n\\nII. RELATED WORK\\n\\nAnomaly detection techniques can be divided into six\\ncategories according to Cook [7]: statistical and probabilistic,\\npattern matching, distance-based, clustering-based, predictive,\\nand finally ensemble. Additionally, Chandola et al [6] di-\\nvide the anomaly detection techniques into supervised, semi-\\nsupervised, and unsupervised based on the training mode\\nemployed in each case. In a similar way, the methods of the lit-\\nerature are categorized into three main groups: statistical meth-\\n\\nods, machine learning methods, and deep learning methods\\n[6]. In these categorizations, statistical methods assume that\\nanomalies are generated from a statistical model. In contrast, in\\nmachine learning methods, the anomaly generation mechanism\\nis considered a black box and anomalies are detected based\\non the data. Finally, the third category comprises techniques\\nthat are based exclusively on neural networks. They are often\\nconsidered part of machine learning so there is often no\\nseparation between the second and third categories.\\n\\nFig. 1. Alternative groupings of anomaly detection techniques.\\n\\nSeveral methods in the literature employ statistical [8]\\nand machine learning techniques [9] to detect anomalies in\\nmultivariate time series. The works that build on statistics rely\\non assumptions about the distribution of data, but their advan-\\ntage is the cheap computation and the straightforwardness in\\nexplaining them. On the other hand, machine learning models\\ncan generalize better, but they have more computationally\\nexpensive training and retraining in case new datasets arrive.\\nThe respective works list several machine learning approaches\\nbased on Multi-layer perceptrons, Support vector machines,\\nand Auto-associator approaches such as the PCA.\\n\\nA grouping of the alternative techniques is depicted in\\nFigure 1. The methodology we propose is based on machine\\nlearning techniques and combines elements both from unsu-\\npervised and semi-supervised anomaly detection. It can be\\nconsidered an ensemble deep learning model as it uses a\\nvariety of neural networks as a base and builds upon them.\\nThe ensemble model makes use of the basic neural networks\\nto discover sequential and spatial relationships in time series.\\nConcerning the nature and distribution of data, we do not make\\nany special assumptions.\\n\\nIn the related literature, we encounter methods that are close\\nto our proposed model, so in the following, we discuss their\\nsimilarities and differences and use them in the experimental\\ncomparison. Chauhan and Vig [10] use the Long Short Term\\nMemory (LSTM) network [11] on an electrocardiography\\nsignals dataset that contains 1-dimension time series to detect\\nanomalies. Malhotra et al. [12] use CNNs on a balanced\\ndataset of healthy and broken tooth gears, to detect anomalies.\\nAutoencoders have been first introduced in [13] and have\\nalso been extended to detect anomalies in [14]. Malhotra\\nin [14] uses a network similar to the LSTM Autoencoder\\non three 1-dimension datasets and one proprietary multiple-\\ndimension dataset. Park in [15] uses an LSTM-Variational\\nAutoencoder (VAE) model for anomaly detection problems\\n\\n\\x0con multidimensional time series. In [16] authors have built\\na collection of multivariate time series, which is used to\\nevaluate anomaly detection algorithms and provide a variety of\\nalgorithms as a baseline. Finally, in [17] a method is proposed\\nthat results in a hybrid network using LSTM and Capsule\\nnetworks, which are evaluated on the SKAB dataset.\\n\\nAlthough many of the above-mentioned methods are avail-\\nable to detect anomalies in [16] we see that the performance\\nthey achieve is not satisfactory and there is significant room\\nfor improvement. In this direction, authors in [18] suggest the\\nselection of subsets of the feature set to take advantage of\\nthe fact that anomalies in high dimensional data are hardly\\ndetected in all the dimensions and can better be detected using\\nonly a subset of features. In the same direction, the authors\\nin [19] employ ensembles and boost their performance by\\nincreasing the diversity of the learners they employ. For this\\npurpose, they propose a technique called Rotation Forest to\\nincrease diversity and accuracy together.\\n\\nAs machine learning methods are evolving, ensemble meth-\\nods emerge, aiming to combine the advantages of the indi-\\nvidual models and methods to capture the different aspects\\nof data. However, ensemble methods are not yet widely used\\nin anomaly detection [20], [21]. Furthermore, the authors in\\n[22] concluded that anomaly detection using an ensemble of\\nmethods and an unsupervised approach has limited value, and\\nsuggested the use of ensemble methods in semi-supervised or\\nsupervised approaches. Therefore, to address the limitations of\\nthe above-mentioned approaches and increase the performance\\nof the anomaly detection task, we first recommend the use\\nof both Feature Bagging [18] and a transformation based on\\nRotations [19] which we call Nested Rotations, simultaneously\\nin an unsupervised setup. To further extend the prediction\\nperformance we also evaluate the use of an ensemble technique\\nthat follows a semi-supervised approach using the afore-\\nmentioned techniques. The Feature Bagging technique allows\\nus to shrink the feature space and predict anomalies based\\nonly on a subset of features. With the transformation based\\non Nested Rotations, we divide the problem sub-space into\\npartitions and rotate them to increase the data diversity. For\\nthis purpose, we employ the PCA method which gives us\\northogonal eigenvectors to use as our new axes.\\n\\nIII. PROBLEM FORMULATION\\n\\nLet X = (Xt : t ∈ T ) be a multivariate time series, where\\nT is the index set and Xt ∈ Rd, ∀t ∈ T . For all t ∈ T we\\nassign a score which is called anomaly score. Let IQR be\\nthe IQR of all anomaly scores the we get during the training\\nphase then, based on a threshold δ = 1.5 ∗ IQR it is decided\\nif Xt is an anomaly when the score is greater than δ or\\nnot an anomaly otherwise. So we denote the anomaly score\\nof Xt as ASc(Xt) and transform this problem into a binary\\nproblem by defining AScbinary(Xt) = 1 if ASc(Xt) > δ and\\nAScbinary(Xt) = 0 otherwise. Every detector has a different\\nscale for anomaly score for a given Xt but all detectors have\\nbinary outcomes (AScbinary(Xt)) when we define a threshold.\\nOur approach defines multiple detectors so let ASci\\nbinary(Xt)\\n\\nbe the binary outcome for the i-th detector for thr Xt instance.\\nThen we could aggregate the anomaly scores of detectors into\\none ASc(Xt) = agg∀i(ASci(Xt)) and then transform it into\\nbinary. The alternative approach, and the one that we use in\\nthis study, is to aggregate the binary outcomes into one binary\\noutcome AScbinary(Xt) = agg∀i(ASci\\n\\nbinary(Xt)).\\n\\nIV. PROPOSED APPROACH\\n\\nAs explained above, we approach the anomaly detection\\nproblem in multivariate time series as a binary classification\\nproblem and rely on an ensemble model and feature engi-\\nneering to improve the classification performance. We assume\\nthat the distribution of data is unknown and thus we depend\\non a representation learning approach to capture the patterns\\nhidden in the multivariate time-series data. For this purpose,\\nwe apply two feature aggregation and a transformation tech-\\nniques, namely Feature Bagging and transformation based\\non Nested Rotation (computed by applying PCA), on five\\ndifferent base architectures (i.e. Autoencoder, Convolutional\\nAutoencoder, LSTM, LSTM Autoencoder and LSTM Vari-\\national Autoencoder) in the fully unsupervised approach. In\\nthe semi-supervised approach, the predictions of the resulting\\nmodels are combined with the help of a Logistic Regressor,\\nwhich is trained to create an efficient ensemble. As depicted\\nin Figures 2 and 3, it is important to decide on the number of\\nmodels to use in the ensemble both in unsupervised learning\\nand semi-supervised learning, and this is affected by the\\nthe different feature subsets and\\nnumber of base models,\\nthe rotations applied to them. In the first step of the semi-\\nsupervised approach, the base architectures that will be used\\nto develop the prediction models must be carefully selected\\nto capture the intrinsic characteristics of the various time\\nseries. In both setups (unsupervised and semi-supervised) it\\nis important to apply the Feature Bagging technique on the\\ndataset and create several subsets, each one comprising a\\nsubset of the original feature, that will be used to train the\\nrespective models. The next step is the Nested Rotations of\\neach subspace (defined by the respective subset of features),\\nwhich is a transformation performed using PCA on partitions\\nof each subspace. The final step in the semi-supervised version\\nis the training of the models with the transformed subsets and\\ntheir integration in an ensemble predictor, based on a Logistic\\nRegression.\\n\\nA. Feature Bagging\\n\\nFeature Bagging has been proposed by Lazarevic and Ku-\\nmar [18] for multidimensional time series as a solution to\\nanomaly detection problems in high-dimensional data. The\\nauthors argued that it is pointless to detect point anomalies\\nbased on the similarity (or distance) in high dimensions since\\nthe respective metrics lose their meaning when the number\\nof dimensions increases. According to the authors, the larger\\nthe number of dimensions, the further the points are separated\\nfrom each other, which results in multiple isolated points that\\nare eventually (but falsely) considered anomalies. In addition,\\nif we consider that most of the dimensions introduce noise,\\n\\n\\x0c5) The final anomaly score of an instance is generated by\\napplying a collection function over the scores derived\\nfrom each model, ASc = agg(ASc1, ASc2, ..., AScM ).\\nWe have selected Majority Voting as the collection\\nfunction in our experiments.\\n\\nB. Feature Bagging with Nested Rotations\\n\\nThe technique has emerged from an algorithm that has been\\ndesigned for classification problems, called Rotation Forest\\n[19]. Rotation Forest extends the popular Random Forest\\nalgorithm, which randomly selects subsets of the original\\nfeature space to train separate decision tree models, which are\\nthen combined in an ensemble. The main novelty of Rotation\\nForest is that it applies a PCA on the randomly selected\\nsubsets of features to get a ”rotation matrix” (the principal\\ncomponents), which is then multiplied by the feature subsets\\nto get the rotated features. The algorithm is described below:\\n\\n1) First the data is normalized. Normalizing the data helps\\nprevent very high values from dominating and influenc-\\ning the result.\\n\\n2) For each dimension the average value is calculated.\\n3) The correlation matrix (covariance matrix) is then cal-\\nculated for all dimensions. That is COV(X, Y ) = 1\\nm ·\\nm\\n(cid:80)\\n(X − µX )(Y − µY ), ∀X, Y where X, Y are two\\ni=1\\ndimensions (two features) and X ̸= Y .\\n\\n4) Then the eigenvectors and eigenvalues of the previous\\ntable are calculated using SVD. These vectors are also\\ncalled Principal Components and the respective values\\nrepresent the ”importance” (or informativeness) of each\\ncomponent.\\n\\n5) We put the vectors in descending order according to the\\neigenvalues. That is, the vector with the largest eigen-\\nvalue is entered first, then the one with the next largest\\neigenvalue etc. Then we choose the k first vectors.\\n6) Finally, we take the matrix with the eigenvectors and\\nmultiply its inverse with the original data matrix to make\\nthe rotation, so:\\n\\n(cid:0)new data(cid:1) = (cid:0)p1\\n\\n... pk\\n\\n(cid:1)T (cid:0)data(cid:1)\\n\\nInstead of keeping only the most ”informative” eigenvectors\\nwe decide to keep all of them. In doing so we rotate our\\ndata according to these axes. To create an ensemble using the\\nrotation method the procedure is as follows:\\n\\n1) Let X = (Xt : t ∈ T ) be a multivariate time series,\\n\\nwhere T is the index set and Xt ∈ Rd, ∀t ∈ T .\\n\\n2) We select the M basic models that will compose the\\nensemble model as before and for each of them we\\nperform the following steps:\\n\\na) We apply the Feature Bagging technique by se-\\nlecting a random number Nm from a uniform\\ndistribution between ⌊ d\\n2 ⌋ and (d − 1) for each of\\nthe models. This step will give us a subset Fm.\\nThe m index denotes the m − Model.\\n\\nFig. 2. Feature Bagging & Nested Rotations\\n\\nFig. 3. Stacking/Fusion Feature Bagging with Nested Rotations models with\\nLogistic Regression\\n\\nthen it\\nis more likely that anomalies will be found in a\\nsubset of the dimensions. So the Feature Bagging algorithm\\nis proposed for creating subspaces, where it would be easier\\nto identify anomalies.\\n\\nThe process for applying feature bagging and feeding the\\n\\nmethod ensemble is as follows:\\n\\n1) Let X = (Xt : t ∈ T ) be a multivariate time series,\\n\\nwhere T is the index set and Xt ∈ Rd, ∀t ∈ T .\\n\\n2) we select several basic (multivariate time series clas-\\nsification) algorithms (e.g. LSTM, autoencoders, etc.)\\nand train a set M of models. Individual models do\\nnot need to use the same architecture, more than one\\nmodel can result from the same algorithm. For each\\nmodel Modelm ∈ {Model1, ..., ModelM } we repeat the\\nfollowing steps:\\n\\n3)\\n\\na) We select a random number Nm, where m denotes\\nthe m − Model, from a uniform distribution be-\\ntween ⌊ d\\n\\n2 ⌋ and (d − 1).\\n\\nb) Then we randomly select a subset Fm of the orig-\\ninal X comprising Nm features without repetition.\\nc) We train the anomaly detection model Modelm on\\n\\nthe Fm subset.\\n\\n4) for each instance, using the set of Model1, ..., ModelM\\nmodels, we get an anomaly score AScm from each\\nmodel for this instance.\\n\\n\\x0cFeature\\nSet\\nF = {F1, ..., F8}\\n\\nFeature Bagging\\nRandom Subset of F\\n{F1, F3, F4, F5, F6, F8}\\n\\npartition subset\\ninto K=2 partions\\n\\nPartion 1\\n{F1, F3, F4}\\n\\nPartion 2\\n{F5, F6, F8}\\n\\nNested\\nRotations\\n\\nRandom\\nSampling 75%\\nApply PCA\\nRotation Matrix:\\nR1\\n\\nRandom\\nSampling 75%\\nApply PCA\\nRotation Matrix:\\nR2\\n\\nTransform\\nData\\nbased on\\nNested Rotations\\n\\nRt =\\n\\n(cid:19)\\n\\n(cid:18)R1\\n\\n0\\n0 R2\\n\\nTransformed Dataset\\nRt × 100% Dataset\\n\\nFig. 4. Nested Rotations\\n\\nb) for\\n\\neach subset Fm we\\n\\ntransfor-\\napply a\\nnested rotations by fur-\\nmation based on\\ninto K subsets such that\\nther partitioning it\\n(cid:83) ... (cid:83) KmK = Fm. We then apply\\n(cid:83) Km2\\nKm1\\nPCA on these subsets as shown in Figure 4.\\nSo each subset Fm is further partitioned into K\\npartitions:\\n\\n• We subsampling the subset Fm in order to\\nincrease diversity of the partitions. This is done\\nbecause if two models m1, m2 happens to have\\nthe same subset Fm1 = Fm2 and happens\\nto have the same partitioning {f1, f2, f3} and\\n{f4, f5, f6} then by subsampling the Fm1 =\\nFm2 resulting into different partitions and thus\\nKm21 ̸= Km11 and Km22 ̸= Km12. So by ap-\\nplying PCA (in the next steps) it is not resulting\\nin the same rotation matrices.\\n• We apply PCA on each partition.\\n• from each partition we get a rotation matrix Rm\\nk ,\\nwhere k denotes the k-partition with dimensions\\n(dimension of Fm)/K ×(dimension of Fm)/K\\n(for simplicity we suppose that the dimension of\\nFm is divided with K but it is not necessary).\\n\\nc) From the previous step (which we called Nested\\nRotations) we have a transformation for each sub-\\n\\nset Fm:\\n\\nRm =\\n\\n\\uf8eb\\n\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\nRm\\n1\\n0\\n...\\n0\\n\\n0\\nRm\\n2\\n...\\n0\\n\\n\\uf8f6\\n\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n\\n· · ·\\n0\\n· · ·\\n0\\n...\\n. . .\\n· · · Rm\\nK\\n\\nwhere each Rm\\nk denotes a submatrix correspond-\\ning to a partition and 0 denotes the zero-submatrix.\\nd) Finally, we get the new (nested rotated) data by\\napplying Rm on the subset Fm. So each the models\\nModel1, ..., ModelM is trained on these ”nested\\nrotated” data.\\neach\\nfor\\n\\nusing\\nModel1, ..., ModelM models, we get\\nscore AScm from each model for this instance.\\n\\nthe\\nof\\nset\\nan anomaly\\n\\ninstance,\\n\\n3) Finally\\n\\n4) The final anomaly score of an instance is generated by\\napplying a collection function over the scores derived\\nfrom each model, ASc = agg(ASc1, ASc2, ..., AScM ).\\nWe have selected Majority Voting as the collection\\nfunction in our experiments.\\n\\nThe number of individual models M, the number of parti-\\ntions K that we split each subset Fm the portion of data that\\nwe use when we subsampling the each Fm to in order to create\\nthe partitions, all these are parameters of this algorithm.\\n\\nC. Stacking Feature Bagging with Nested Rotations models\\n\\nIn this section, we make use of the previous technique to\\nboost the performance of our prediction in a semi-supervised\\nsetup using a Logistic Regressor to combine the individual\\nmodels as a collection function instead of the Majority Voting\\nwe used in the unsupervised setup.\\n\\nLet X = (Xt : t ∈ T ) be a multivariate time series, where\\nT is the index set and Xt ∈ Rd, ∀t ∈ T . We select the\\nM basic models that will compose the ensemble model as\\nbefore and for each of them we perform the following steps:\\nThe procedure for implementing this ensemble method is as\\nfollows:\\n\\n1) First, we divide the training set into two training subsets,\\none (subset A) for training the individual models and one\\n(subset B) for training the Logistic Regressor. This split\\nguarantees that there will be no leak of information be-\\ntween the individual models and the Logistic Regressor.\\n2) In the next step we choose the number of individual\\nmodels M we need to create using the Feature Bagging\\ntechnique combined with Nested Rotations as in the\\nprevious section.\\n\\n3) With these individual models (trained on data set A) we\\nmake predictions on the data set B that the models have\\nnot been trained on.\\n\\n4) As a final step we train a Logistic Regression to improve\\nthe prediction performance. More precisely a prediction\\nis made on every element of data set B, and thus every\\nelement gets T anomaly scores. So a new data set created\\nB′ = b1, b2, b3 where bi = (ASci\\nT )\\nwhere ASci\\nt is the anomaly score that model Modelm\\ngave to this element bi. So at the final semi-supervised\\n\\n2, ..., ASci\\n\\n1, ASci\\n\\n\\x0cstep, we train a Logistic Regression model in data set B′\\ngiven the labels, to learn how to combine the individual\\npredictions.\\n\\nThe number of individual models M, the number of par-\\ntitions K that we split each subset Fm the portion of data\\nthat we use when we subsampling the each Fm to in order to\\ncreate the partitions, all these are parameters of this algorithm\\nwhich derived from the Feature Bagging with Nested Rotations\\nalgorithm.\\n\\nV. EXPERIMENTAL EVALUATION\\n\\nA. Experimental setup\\n\\nThe proposed models of anomaly detection in multivari-\\nate time series with ensemble techniques are implemented\\nand evaluated in Python programming language using the\\nframeworks NumPy, pandas, Scikit-learn, TensorFlow 2, and\\nthe Keras API. Specifically, we used five basic architectures\\nwhich are Autoencoder, LSTM, LSTM Autoencoder, LSTM\\nVariational Autoencoder, and Convolutional Autoencoder. The\\nexperiments took place in the Google Colab environment ex-\\ncept for the model that was proposed in [17] called LSTMcaps.\\nThe datasets we used are taken from Skoltech Anomaly\\nBenchmark (SKAB) [16], which is a collection of time series\\nproduced by the operation of a water pump device (motor)\\nmonitored by various sensors. The set of sensors generates\\neight values at every moment thus resulting in multivariate\\ntime series with eight features, and each monitoring file con-\\ntains normal operation and anomaly points. SKAB therefore\\ncan be used to evaluate techniques and models in the context of\\nAnomaly Detection research in multivariate time series. SKAB\\ncontains 34 multivariate time series of 37401 moments in total\\nor 1100.02 moments per time series on average. Their size is\\n3.41 MB in total or 102.76 KB per time series on average.\\nThey do not contain missing values and they do not contain\\nduplicates. Each point of the time series is a vector of 10\\nvalues which are, the timestamp (representing the exact time\\nwhen the following values were recorded), the anomaly label,\\nand the 8 values that sensors recorded (Accelerometer1RMS,\\nAccelerometer2RMS, Current, Pressure, Temperature, Ther-\\nmocouple, Voltage, and Volume Flow RateRMS). Each time\\nseries is a recorded experiment that starts in a normal state\\nand after a while by switching valves, anomalies are injected.\\nThe evaluation metrics that we report are the F1-Score and\\nArea Under Curve (AUC) of the receiver operating character-\\nistic curve (ROC curve), which is more appropriate for a task\\nthat is by definition highly unbalanced between the majority\\n(normal) and the minority (anomaly) class.\\n\\nThe AUC is defined as follows: Let TPR = T P\\nP\\n\\nand\\nFPR = F N\\nN be the true positive rate and the false positive\\nrate respectively, where TP stands for the true positive cases,\\nP for all the positive cases, FN for the false negatives and N\\nfor all the negative cases. The anomalous points are consid-\\nered positives and the non-anomalous points are considered\\nnegatives. Then the AUC metric is the area under the curve\\ndefined by ROC curve as plotted from TPR and FPR pairs\\n\\nfor various δ ∈ R where δ is the threshold of our anomaly\\ndetection method. F1-Score defined as F1 = 2·Precision·Recall\\nPrecision+Recall\\nwhere Precision = TP\\nTP+FN . For both\\nmetrics, higher values denote better performance. These two\\nmetrics are widely used to evaluate anomaly detection methods\\n(e.g. in [16], [17] and in [23], [24]).\\n\\nPrecision = TP\\n\\nTP+FP ,\\n\\nB. Results\\n\\nIn Table I we can see the comparative results of the\\nproposed ensemble methods and other baseline approaches.\\nEvery section consists of the architecture the ensemble is based\\non. We further divide each section and show the F1 Score\\nand AUC regarding 3 types of models of the corresponding\\narchitecture, the plain version of the model, the ensemble using\\nFeature Bagging, and the ensemble using Feature Bagging\\nwith Nested Rotations. We also included LSTMcaps using the\\nF1 Score that the authors provided in their work. Furthermore,\\nwe extend the table and added a section on top of the table for\\nour semi-supervised approach. For each architecture and for\\neach time series we created and train each model as follows:\\n\\n1) Split/slice the time series into two parts.\\n2) Train the corresponding model with the first part of the\\ntime series. For the ”mixed” section we used 3 parts,\\nthe first two for training and the third part for testing as\\ndescribed in the subsection ”Stacking Feature Bagging\\nwith Nested Rotations models”.\\n\\na) for the Feature Bagging model we created 17\\n\\nmodels of the corresponding architecture.\\n\\nb) for the Feature Bagging with Nested Rotations\\nmodel we created 17 models of the corresponding\\narchitecture. For the parameters of all models, we\\nset the proportion of data to be 75%, and the K\\npartitions on which the PCA algorithm is applied\\nwas set to 2.\\n\\nc) for the architecture ”mixed” we used 60 Feature\\nBagging with Nested Rotations (12 for each archi-\\ntecture) using the first part of the time series and\\ncombined them with a Logistic Regressor using the\\nsecond part of the time series. For the parameters\\nof all models, we set the proportion of data to be\\n75%, the K subset on which the PCA algorithm is\\napplied was set to 2.\\n\\n3) Test the corresponding model using the second part of\\n\\nthe time series:\\n\\na) for the Feature Bagging model of the correspond-\\ning architecture every instance/moment of the time\\nseries has 17 labels of 0/1 and we use Majority\\nVoting as an aggregation function to conclude to a\\nsingle 0/1 label.\\n\\nb) for the Feature Bagging with Nested Rotations\\nmodel of the corresponding architecture using the\\nsame logic as above.\\n\\nc) for the architecture ”mixed” we used the third part\\n\\nof the time series for testing.\\n\\n\\x0cAt\\nthe end of this process, we use the macro-average to\\nconclude the F1 Score and AUC for every architecture and\\nits corresponding models. This evaluation approach has been\\nfollowed by other researchers in similar tasks (e.g. [16] and\\n[17]) to evaluate their models.\\n\\nIn Table I that follows, we see that Feature Bagging can\\nslightly improve the performance of most base methods. More\\nspecifically, the use of Feature Bagging improved the best\\nperformance of the plain versions of autoencoder, LSTM\\nautoencoder and LSTM VAE, whereas couldn’t beat the best\\nperformance of the plain versions of a convolutional autoen-\\ncoder and the LSTM model without Feature Bagging. This is\\nan indication that Feature Bagging captures anomalies even if\\nthey are hidden in a subset of features. To further increase the\\nperformance then we combine Feature Bagging and Nested\\nRotations and again we combine the results with majority\\nvoting. We can clearly see that\\nthese two techniques in\\ncombination provide us with around 2-4% better performance\\nthan the plain version of models. The only model that beats the\\nperformance of Feature Bagging with Rotation is the LSTM.\\nAs we can see the best model in the unsupervised setup\\nis when we used the Convolutional Autoencoder architecture\\napplying Feature Bagging with Nested Rotations techniques\\nresulting in an ensemble using Majority Voting. This model\\nhas a 0.7873 F1 Score and 0.8315 AUC which outperforms the\\nothers. Finally as expected in the semi-supervised environment\\nthe performance is boosted, and the improvement is at least\\n10%.\\n\\nC. Discussion\\n\\nFrom the results of the tables I, we see that Feature Bagging\\ncan improve the performance of some models but not all of\\nthem. However, when it is combined with nested rotations,\\nthe performance is usually better. More specifically, when we\\ncombine it with the transformation based on nested rotations\\ncomputed by PCA into the Feature Bagging with Nested\\nRotations technique we have an overall improvement of 2%.\\nThe fact that every Feature Bagging with Nested Rotations\\nperforms better against\\nthe majority of the corresponding\\narchitectures leads us to the conclusion that this ensemble\\ntechnique can take anomaly detection a step further. Also,\\nwe can conclude that Feature Bagging and Nested Rotations\\nact complementary: Feature Bagging reveals the anomalies\\nand the features that are most affected, whereas Nested\\nRotations inject diversity and boost the performance of the\\nensemble. Integrating multiple PCAs with feature bagging\\nleads to increased effectiveness in most cases, as can be seen\\nfrom the results in Table I. One possible explanation for\\nthis is that multiple PCAs after random subspace selection\\nfurther increases the diversity of the individual classifiers in\\nthe ensemble. A deeper investigation of the effect of PCA on\\nthe model ensemble is left as future work.\\n\\nOn the other hand, the semi-supervised ensemble model\\nwhich combines Feature Bagging with Nested Rotations on the\\nindividual models, using a Logistic Regressor, gives us the best\\nresults on the anomaly detection problem and outperforms all\\n\\nArchitecture\\n(mode)\\nMixed (semi-\\nsupervised)\\n\\nConvolutional\\nAutoencoder\\n(unsupervised)\\n\\nLSTM\\nAutoencoder\\n(unsupervised)\\n\\nLSTMCaps\\n(unsupervised)\\n\\nLSTM\\n(unsupervised)\\n\\nLSTM\\nVariational\\nAutoencoder\\n(unsupervised)\\n\\nAutoencoder\\n(unsupervised)\\n\\nTABLE I\\nMODEL COMPARISON\\n\\nModel Title\\n\\nStacking FBR models\\nwith Logistic Regression\\n\\nFeature Bagging with\\nNested Rotations\\n\\nF1\\nScore\\n\\n0.85\\n\\nAUC\\n\\n0.88\\n\\n0.7873\\n\\n0.8315\\n\\nFeature Bagging\\n\\n0.7451\\n\\n0.80\\n\\nPlain\\n\\n0.7622\\n\\n0.8117\\n\\nFeature Bagging with\\nNested Rotations\\n\\n0.7641\\n\\n0.8190\\n\\nFeature Bagging\\n\\n0.7465\\n\\n0.8050\\n\\nPlain\\n\\n0.7410\\n\\n0.8021\\n\\nLSTMCaps\\n\\n0.74\\n\\n-\\n\\nFeature Bagging with\\nNested Rotations\\n\\n0.7074\\n\\n0.7749\\n\\nFeature Bagging\\n\\n0.6723\\n\\n0.7500\\n\\nPlain\\n\\n0.7225\\n\\n0.7867\\n\\nFeature Bagging with\\nNested Rotations\\n\\n0.7014\\n\\n0.7704\\n\\nFeature Bagging\\n\\n0.6978\\n\\n0.7680\\n\\nPlain\\n\\n0.6815\\n\\n0.7570\\n\\nFeature Bagging with\\nNested Rotations\\n\\n0.6259\\n\\n0.7231\\n\\nFeature Bagging\\n\\n0.5999\\n\\n0.7089\\n\\nPlain\\n\\n0.5935\\n\\n0.7050\\n\\nother methods. However, its main limitation is that it needs\\na few training samples in order to learn how to combine\\nindividual models. It also needs much more time for training\\nand inference since it produces multiple models. This is due\\nto the fact that apart from the 60 learners it employs, the PCA\\nalgorithm is very time-consuming in its computations. Since\\nPCA is applied many more times than the number of models in\\nthe ensemble the computational cost can significantly increase.\\nHowever, the process can be easily parallelized using a new\\nthread for each model and a multi-core system to do the\\ntraining or inference.\\n\\nAbout the ensemble models (either the unsupervised or\\nthe one in semi-supervised) the final number of learners as\\nwell as the other parameters of algorithms has been chosen\\nafter performing a grid-like search that balances between the\\navailable architectures and the number of models to train for\\neach architecture and the other parameters while respecting\\nour limitations in time and computational resources.\\n\\n\\x0cVI. CONCLUSIONS AND NEXT STEPS\\n\\nIn this work, we implemented two ensemble techniques\\napplying in unsupervised mode and semi-supervised mode\\nfor anomaly detection in multivariate time series. This prob-\\nlem has been approached by various algorithms both from\\nmachine techniques and deep learning models. We used 5\\nbasic deep machine learning models and built on them to\\nimplement ensemble techniques to achieve even better results.\\nThe ensemble models built initially were using two techniques\\ncalled Feature Bagging and a transformation based on Nested\\nRotations computed by PCA. The result showed us that these\\ntechniques could improve our basic models in combination\\nwith unsupervised learning. Feature Bagging alone had almost\\nthe same performance as basic models in some cases. When\\nwe combined it with Nested Rotations using PCA algorithm,\\nin most cases, performed better than the basic models. Finally,\\nwhen we combined all of the above into a general model\\ncalled Stacking/Fusion Feature Bagging with Rotation using\\na Logistic Regressor in semi-supervised mode and we got\\nthe previous techniques as\\nthe best performance from all\\nexpected. Ensemble techniques are known to greatly improve\\nperformance and solve many problems and our results showed\\nthat they can also help a lot in detecting anomalies.\\n\\nAlthough these models are performing quite well in the\\nanomaly detection problem in an unsupervised or semi-\\nsupervised environment they heavily depend on the aggrega-\\ntion function applied to the multiple scores we get at the final\\nstage. Hence it is a valid question whether another function\\nor even a non-linear model can boost the performance further.\\nIn [25] they did research on variants of majority voting and\\npresented the two veto-based voting schemes that can combine\\nmultiple classifiers together based on their reliability of them.\\nAnother important point for future work is to test our model\\nin higher dimensions. The two techniques we used Feature\\nBagging and Rotation are the core of our models. As we\\ndiscussed earlier Feature Bagging is robust in high dimensions\\nwhile Rotation injects diversity and boosts performance. Thus,\\nwe believe that as the dimensionality increases, the model\\nshould not be affected, but more tests with time series in higher\\ndimensions need to be done to confirm this. Furthermore, as\\nwe already mentioned PCA seems to help a lot but a deeper\\ninvestigation of the effect of PCA on the model ensemble is\\nleft as future work.\\n\\nACKNOWLEDGMENT\\n\\nThe work leading to these results has received funding\\nfrom the European Union’s Horizon 2020 research and innova-\\ntion programme under Grant Agreement No. 965231, project\\nREBECCA (REsearch on BrEast Cancer induced chronic\\nconditions supported by Causal Analysis of multi-source data).\\n\\nREFERENCES\\n\\n[1] M. Braei and S. Wagner, “Anomaly detection in univariate time-series: A\\nsurvey on the state-of-the-art,” arXiv preprint arXiv:2004.00433, 2020.\\n[2] A. B. Nassif, M. A. Talib, Q. Nasir, and F. M. Dakalbab, “Machine\\nLearning for Anomaly Detection: A Systematic Review,” IEEE Access,\\nvol. 9, pp. 78 658–78 700, 2021.\\n\\n[3] G. Mamalakis, C. Diou, A. L. Symeonidis, and L. Georgiadis, “Of\\ndaemons and men: reducing false positive rate in intrusion detection\\nsystems with file system footprint analysis,” Neural Computing and\\nApplications, vol. 31, pp. 7755–7767, 2019.\\n\\n[4] I. Ullah and Q. H. Mahmoud, “Design and Development of RNN\\nAnomaly Detection Model for IoT Networks,” IEEE Access, vol. 10,\\npp. 62 722–62 750, 2022.\\n\\n[5] P. Gogoi, D. K. Bhattacharyya, B. Borah, and J. K. Kalita, “A survey\\nof outlier detection methods in network anomaly identification,” The\\nComputer Journal, vol. 54, no. 4, pp. 570–588, 2011.\\n\\n[6] V. Chandola, A. Banerjee, and V. Kumar, “Anomaly detection: A survey,”\\nACM computing surveys (CSUR), vol. 41, no. 3, pp. 1–58, 2009.\\n[7] A. A. Cook, G. Mısırlı, and Z. Fan, “Anomaly detection for iot time-\\nseries data: A survey,” IEEE Internet of Things Journal, vol. 7, no. 7,\\npp. 6481–6494, 2020.\\n\\n[8] M. Markou and M. Singh, “Novelty detection: A review—part 1:\\nStatistical approaches,” Signal Processing, vol. 83, pp. 2481–2497, 12\\n2003.\\n\\n[9] M. Markou and S. Singh, “Novelty detection: a review—part 2:: neural\\nnetwork based approaches,” Signal processing, vol. 83, no. 12, pp. 2499–\\n2521, 2003.\\n\\n[10] S. Chauhan and L. Vig, “Anomaly detection in ecg time signals via deep\\nlong short-term memory networks,” in 2015 IEEE Conference on Data\\nScience and Advanced Analytics (DSAA).\\n\\nIEEE, 2015, pp. 1–7.\\n\\n[11] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\\n\\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\\n\\n[12] S. Mukhopadhyay and M. Litoiu, “Fault detection in sensors using\\nsingle and multi-channel weighted convolutional neural networks,” in\\nProceedings of the 10th International Conference on the Internet of\\nThings, 2020, pp. 1–8.\\n\\n[13] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal\\nrepresentations by error propagation,” University of California, San\\nDiego, Tech. Rep., 1985.\\n\\n[14] P. Malhotra, A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal, and\\nG. Shroff, “Lstm-based encoder-decoder for multi-sensor anomaly de-\\ntection,” arXiv preprint arXiv:1607.00148, 2016.\\n\\n[15] D. Park, Y. Hoshi, and C. C. Kemp, “A multimodal anomaly detector\\nfor robot-assisted feeding using an lstm-based variational autoencoder,”\\nIEEE Robotics and Automation Letters, vol. 3, no. 3, pp. 1544–1551,\\n2018.\\n\\n[16] I. D. Katser and V. O. Kozitsin, “Skoltech anomaly benchmark (skab),”\\n\\nhttps://www.kaggle.com/dsv/1693952, 2020.\\n\\n[17] A. Elhalwagy and T. Kalganova, “Hybridization of capsule and lstm\\nnetworks for unsupervised anomaly detection on multivariate data,”\\narXiv preprint arXiv:2202.05538, 2022.\\n\\n[18] A. Lazarevic and V. Kumar, “Feature bagging for outlier detection,” in\\nbooktitle=11th ACM SIGKDD International Conference on Knowledge\\nDiscovery in Data Mining,, vol. 21, 01 2005, pp. 157–166.\\n\\n[19] J. Rodr´ıguez, L. Kuncheva, and C. Alonso, “Rotation forest: A new\\nclassifier ensemble method,” IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, vol. 28, pp. 1619–30, 11 2006.\\n\\n[20] C. C. Aggarwal, “Outlier ensembles: position paper,” ACM SIGKDD\\n\\nExplorations Newsletter, vol. 14, no. 2, pp. 49–58, 2013.\\n\\n[21] A. Zimek, R. J. Campello, and J. Sander, “Ensembles for unsupervised\\noutlier detection: challenges and research questions a position paper,”\\nAcm SIGKDD Explorations Newsletter, vol. 15, no. 1, pp. 11–22, 2014.\\n[22] A. Chiang, E. David, Y.-J. Lee, G. Leshem, and Y.-R. Yeh, “A study on\\nanomaly detection ensembles,” Journal of Applied Logic, vol. 21, pp.\\n1–13, 2017.\\n\\n[23] M. Munir, S. A. Siddiqui, A. Dengel, and S. Ahmed, “Deepant: A deep\\nlearning approach for unsupervised anomaly detection in time series,”\\nIEEE Access, vol. 7, pp. 1991–2005, 2018.\\n\\n[24] M. Goldstein and S. Uchida, “A comparative evaluation of unsupervised\\nanomaly detection algorithms for multivariate data,” PloS One, vol. 11,\\nno. 4, p. e0152173, 2016.\\n\\n[25] R. K. Shahzad and N. Lavesson, “Comparative analysis of voting\\nschemes for ensemble-based malware detection,” Journal of Wireless\\nMobile Networks, Ubiquitous Computing, and Dependable Applications,\\nvol. 4, no. 1, pp. 98–117, 2013.\\n\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}